---
title: "Prediction Assignment Writeup"
author: "Punit Kapoor"
date: "9/23/2021"
output: 
  html_document: 
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```
# Setting working directory
setwd("~/R/Practical Machine Learning")

# Loading required R packages and setting a seed
library(lattice)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(corrplot)
library(rattle)
library(randomForest)
library(RColorBrewer)

set.seed(222)

# Loading data for training and test datasets
url_train <- "pml-training.csv"
url_quiz  <- "pml-testing.csv"

data_train <- read.csv(url_train, strip.white = TRUE, na.strings = c("NA",""))
data_quiz  <- read.csv(url_quiz,  strip.white = TRUE, na.strings = c("NA",""))

dim(data_train)
dim(data_quiz)

# Creating two partitions (75% and 25%) within the original training dataset
in_train  <- createDataPartition(data_train$classe, p=0.75, list=FALSE)
train_set <- data_train[ in_train, ]
test_set  <- data_train[-in_train, ]

dim(train_set)
dim(test_set)

# Since two datasets (train_set and test_set) have a large number of NA values as well as near-zero-variance (NZV) variables, removing them with their ID variables
nzv_var <- nearZeroVar(train_set)

train_set <- train_set[ , -nzv_var]
test_set  <- test_set [ , -nzv_var]

dim(train_set)
dim(test_set)

na_var <- sapply(train_set, function(x) mean(is.na(x))) > 0.95
train_set <- train_set[ , na_var == FALSE]
test_set  <- test_set [ , na_var == FALSE]

dim(train_set)
dim(test_set)

# Since columns 1 to 5 are identification variables only, they will be removed as well
train_set <- train_set[ , -(1:5)]
test_set  <- test_set [ , -(1:5)]


# Correlation Analysis
# Correlation analysis between the variables before the modeling work itself is done. The “FPC” is used as the first principal component order.
corr_matrix <- cor(train_set[ , -54])
corrplot(corr_matrix, order = "FPC", method = "circle", type = "lower", tl.cex = 0.6, tl.col = rgb(0, 0, 0))

# If two variables are highly correlated their colors are either dark blue (for a positive correlation) or dark red (for a negative correlations). Because there are only few strong correlations among the input variables, the Principal Components Analysis (PCA) will not be performed in this analysis. Instead, a few different prediction models will be built to have a better accuracy.

# Prediction Models
# Decision Tree Model
set.seed(2222)
fit_decision_tree <- rpart(classe ~ ., data = train_set, method="class")
fancyRpartPlot(fit_decision_tree)

# Predictions of the decision tree model on test_set.
predict_decision_tree <- predict(fit_decision_tree, newdata = test_set, type="class")
conf_matrix_decision_tree <- confusionMatrix(predict_decision_tree, factor(test_set$classe))
conf_matrix_decision_tree

# The predictive accuracy of the decision tree model is relatively low at 75.2 %.
plot(conf_matrix_decision_tree$table, col = conf_matrix_decision_tree$byClass, 
     main = paste("Decision Tree Model: Predictive Accuracy =",
                  round(conf_matrix_decision_tree$overall['Accuracy'], 4)))

# Generalized Boosted Model (GBM)
set.seed(2222)
ctrl_GBM <- trainControl(method = "repeatedcv", number = 5, repeats = 2)
fit_GBM  <- train(classe ~ ., data = train_set, method = "gbm",
                  trControl = ctrl_GBM, verbose = FALSE)
fit_GBM$finalModel

# Predictions of the GBM on test_set
predict_GBM <- predict(fit_GBM, newdata = test_set)
conf_matrix_GBM <- confusionMatrix(predict_GBM, factor(test_set$classe))
conf_matrix_GBM

# The predictive accuracy of the GBM is relatively high at 98.57 %.

# Random Forest Model
set.seed(2222)
ctrl_RF <- trainControl(method = "repeatedcv", number = 5, repeats = 2)
fit_RF  <- train(classe ~ ., data = train_set, method = "rf",
                  trControl = ctrl_RF, verbose = FALSE)
fit_RF$finalModel

# Predictions of the random forest model on test_set.
predict_RF <- predict(fit_RF, newdata = test_set)
conf_matrix_RF <- confusionMatrix(predict_RF, factor(test_set$classe))
conf_matrix_RF

# The predictive accuracy of the Random Forest model is excellent at 99.8 %.

# Applying the Best Predictive Model to the Test Data
# The following are the predictive accuracy of the three models:

# Decision Tree Model: 75.20 %
# Generalized Boosted Model: 98.57 %
# Random Forest Model: 99.80 %

# The Random Forest model is selected and applied to make predictions on the 20 data points from the original testing dataset (data_quiz).

predict_quiz <- as.data.frame(predict(fit_RF, newdata = data_quiz))
predict_quiz

```